# WIDS 2025: Deep Learning From Scratch

---

## Week 1: 

Establish the mathematical and coding foundations required for the project, along with an introduction to Kaggle-style machine learning workflows.

### ðŸ”¹ Linear Algebra 

- Importance of linear algebra in machine learning
- Vectors as points and directions
- Linear transformations and geometric meaning of matrices

### ðŸ”¹ NumPy & Pytorch Basics

- Introduction to PyTorch tensors and their relation to NumPy arrays
- Tensor creation, shapes, indexing, and slicing
- Basic operations and broadcasting

### ðŸ”¹ Distance Computations

- Efficient computation of distance matrices using NumPy
- Understanding vectorization vs Python loops
- Useful preparation for Kaggle-style problems

### ðŸ”¹ Kaggle Introduction

- Overview of Kaggle competitions and workflows
- Understanding datasets (train.csv, test.csv) and submissions
- Explored the Titanic dataset as a first contest

---

## Week 2: 

### ðŸ”¹ Perceptron

- Perceptron as a linear binary classifier
- Role of weights, bias, and activation functions
- Learning a linear decision boundary
- Limitations of linear models

### ðŸ”¹ XOR Problem

- Why a single-layer perceptron fails on XOR
- Motivation for multi-layer networks
- Geometric intuition behind non-linear separability

### ðŸ”¹ Neural Networks (MLPs)

- Extending perceptrons to multi-layer neural networks
- Role of hidden layers and non-linear activations
- Learning complex decision boundaries

### ðŸ”¹ Backpropagation

- Backpropagation as the core learning algorithm
- Gradient computation using the chain rule
- Weight and bias updates to minimize loss
- Focus on mathematical understanding

---
I have attached the coding part of week 1. Few datasets have not been uploaded due to size limit of github. No coding was done in week 2 as it focussed on theorotical part of ml. Though it involved some interactive learning.
---

